{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12bedd1a",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ“˜ Part 4: Chain Rule for Machine Learning\n",
    "\n",
    "This notebook explains the **Chain Rule**, the mathematical idea that makes\n",
    "**backpropagation and neural network training possible**.\n",
    "\n",
    "Focus:\n",
    "- Intuition\n",
    "- Flow of gradients\n",
    "- ML relevance\n",
    "\n",
    "No deep neural network math, only foundations.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4c8f4a",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Why the Chain Rule Matters in ML\n",
    "\n",
    "In Machine Learning:\n",
    "- Models are built from **layers of functions**\n",
    "- Loss depends on parameters **indirectly**\n",
    "\n",
    "Chain Rule answers:\n",
    "> *How does a small change in an early parameter affect the final loss?*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644d41b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5445b8",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Simple Function Composition\n",
    "\n",
    "Consider:\n",
    "y = f(x)  \n",
    "z = g(y)\n",
    "\n",
    "Then:\n",
    "z = g(f(x))\n",
    "\n",
    "The output depends on x **through y**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0cee0f",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Chain Rule (Core Idea)\n",
    "\n",
    "If:\n",
    "z = g(y) and y = f(x)\n",
    "\n",
    "Then:\n",
    "dz/dx = (dz/dy) Â· (dy/dx)\n",
    "\n",
    "This is the **Chain Rule**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551cf6f2",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Simple Numerical Example\n",
    "\n",
    "Let:\n",
    "y = xÂ²  \n",
    "z = 3y + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50ccb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "def g(y):\n",
    "    return 3*y + 1\n",
    "\n",
    "x = 2\n",
    "\n",
    "dy_dx = 2*x\n",
    "dz_dy = 3\n",
    "\n",
    "print(\"dz/dx via chain rule:\", dz_dy * dy_dx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d60e2c",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Why We Need Chain Rule in ML\n",
    "\n",
    "Loss functions depend on:\n",
    "- Predictions\n",
    "- Predictions depend on parameters\n",
    "\n",
    "So:\n",
    "Loss â†’ Prediction â†’ Parameter\n",
    "\n",
    "Chain Rule allows gradients to **flow backward**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15809f7",
   "metadata": {},
   "source": [
    "\n",
    "## 6. ML-style Example (Single Parameter)\n",
    "\n",
    "Prediction:\n",
    "yÌ‚ = w Â· x\n",
    "\n",
    "Loss:\n",
    "L = (yÌ‚ âˆ’ y)Â²\n",
    "\n",
    "Loss depends on w through yÌ‚.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb854a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = 2\n",
    "y = 5\n",
    "w = 1.0\n",
    "\n",
    "y_hat = w * x\n",
    "loss = (y_hat - y)**2\n",
    "\n",
    "dL_dyhat = 2*(y_hat - y)\n",
    "dyhat_dw = x\n",
    "\n",
    "print(\"Gradient using chain rule:\", dL_dyhat * dyhat_dw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb8c965",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Backpropagation Intuition\n",
    "\n",
    "Backpropagation is:\n",
    "- Repeated application of the chain rule\n",
    "- From loss back to parameters\n",
    "- Layer by layer\n",
    "\n",
    "Neural networks are just **many chain rules stacked together**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584ecfa8",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Computational Graph View\n",
    "\n",
    "Think of models as graphs:\n",
    "- Nodes = operations\n",
    "- Edges = flow of values\n",
    "\n",
    "Gradients flow **backward** through the graph.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53faab5",
   "metadata": {},
   "source": [
    "\n",
    "## 9. ML Interpretation (Very Important)\n",
    "\n",
    "Chain Rule:\n",
    "- Enables gradient computation\n",
    "- Makes deep learning possible\n",
    "- Works efficiently via backpropagation\n",
    "\n",
    "Without chain rule:\n",
    "- No neural networks\n",
    "- No deep learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233019ea",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸ§ª Practice (Thinking-Based)\n",
    "\n",
    "1. Why canâ€™t we differentiate loss directly w.r.t parameters?\n",
    "2. How does chain rule simplify complex models?\n",
    "3. Why is backpropagation efficient?\n",
    "4. How does this scale to deep networks?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ef2895",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸ“Œ Calculus for ML â€“ Completion\n",
    "\n",
    "You have now completed the **core calculus required for Machine Learning**:\n",
    "- Derivatives\n",
    "- Partial derivatives\n",
    "- Gradients & gradient descent\n",
    "- Chain rule (backpropagation intuition)\n",
    "\n",
    "Next, we will **apply this math directly to ML algorithms**.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}