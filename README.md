# ğŸ“˜ Calculus for Machine Learning

This repository contains my **personal learning notes and Jupyter notebooks**
focused on the **calculus foundations required for Machine Learning**.

The goal is to build **strong intuition** for how ML models learn using:
- derivatives
- gradients
- gradient descent
- chain rule (backpropagation intuition)

All explanations are **intuition-first** and supported by **NumPy-based experiments**.

---

## ğŸ“‚ Repository Structure
```
calculus-for-ml/
â”‚
â”œâ”€â”€ notebooks/
â”‚ â”œâ”€â”€ Part1_Derivatives_for_ML.ipynb
â”‚ â”œâ”€â”€ Part2_Partial_Derivatives_for_ML.ipynb
â”‚ â”œâ”€â”€ Part3_Gradients_and_Gradient_Descent.ipynb
â”‚ â””â”€â”€ Part4_Chain_Rule_for_ML.ipynb
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ .gitignore
â””â”€â”€ LICENSE

---
```

## ğŸ¯ Learning Objectives

- Understand how derivatives guide learning
- See gradients as vectors of change
- Learn why gradient descent works
- Build intuition for backpropagation

---

## âš ï¸ Disclaimer

This repository represents my **personal learning journey**.
It is not intended as a textbook or formal research work.
